~~
~~ Copyright 2008 FatWire Corporation. All Rights Reserved.
~~
~~ Licensed under the Apache License, Version 2.0 (the "License");
~~ you may not use this file except in compliance with the License.
~~ You may obtain a copy of the License at
~~
~~    http://www.apache.org/licenses/LICENSE-2.0
~~
~~ Unless required by applicable law or agreed to in writing, software
~~ distributed under the License is distributed on an "AS IS" BASIS,
~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
~~ See the License for the specific language governing permissions and
~~ limitations under the License.
~~

SS-CRAWLER

	This program will crawl ContentServer for all the pagelets 
	and links it finds, and report on them. Please see the 
	reporting section on the reports provided.
	
	It can be useful to understand the implemented pagelet and caching strategy on a site.

USAGE

    ss-crawler \<subcommand\> [options] [argument]

	minimal arguments to specify for windows
	
	<<<ss-crawler.bat crawler \<full url to ContentServer with querystring containing parameters for staring page\>>>>
	
	for instance: crawl FSII at localhost
	 
	<<<ss-crawler.bat crawler "http://localhost:8080/cs/ContentServer?pagename=FSIIWrapper&cid=1118867611403&c=Page&p=1118867611403&childpagename=FirstSiteII/FSIILayout">>>
	
	minimal arguments to specify for UNIX (bash)
	
	<<<ss-crawler.sh crawler \<full url to ContentServer with querystring containing parameters for staring page\>>>>
	
	for instance: crawl FSII at localhost
	 
	<<<ss-crawler.sh crawler 'http://localhost:8080/cs/ContentServer?pagename=FSIIWrapper&cid=1118867611403&c=Page&p=1118867611403&childpagename=FirstSiteII/FSIILayout'>>>

COMMAND LINE ARGUMENTS

	usage: ss-crawler \<subcommand\> [options] [argument]
	
	OPTIONS: [-d \<dir\>] [-f \<classname\>] [-h] [-m \<num\>] [-ph \<host\>]
       [-pp \<port\>] [-pu \<username\>] [-pw \<password\>] [-t \<num\>]

	ARGUMENT: the start uri in the form of 'http://localhost:8080/cs/ContentServer?pagename=...'.

	Available subcommands:
	
	* crawler: extensive reporting on the discovered pagelets.
	
	* warmer:  warm the cache with reporting of PageRenderTimeReporter.

    []
    	
-----------
 -d,--reportDir <dir>                Directory where reports are stored

 -f,--uriHelperFactory <classname>   a classname extending com.fatwire.dta.sscrawler.util.UriHelperFactory 
                                     if you need a special implementation for uri parsing>. 
                                     Normally no need to set this. 
                                     For CS6.2 you need to set it to com.fatwire.dta.sscrawler.util.DecodingUriHelperFactory.

 -h,--help                           print this message.

 -m,--max <num>                      Maximum number of pages, default is
                                     unlimited

 -ph,--proxyHost <host>              Proxy hostname

 -pp,--proxyPort <port>              Proxy port number

 -pu,--proxyUsername <username>      Proxy Username

 -pw,--proxyPassword <password>      Proxy Password

 -t,--threads <num>                  Number of concurrent threads that are
                                     reading from ContentServer

-----------------	



REPORTS

	Several reports will be created when this program is running.
	
	* Summary: summary.txt
	
	  Gives a RED/AMBER/GREEN report for a quick overview of the status. For any RED or AMBER status you should check the individual report for more detail. 

	* LinkCollectingReporter: pagelets.txt

	  A list of pagenames and the various other query parameters 
	  
	* OuterLinkCollectingReporter: browsable-links.txt

	  A list of links as that could be used by a browser. In technical terms: the link to the outer wrapper pagelet.
	  
	* PageCollectingReporter: pages-list.txt

	  Writes the full set of http headers and response body into a subdir called pages, 
	  and procudes a overview file (pages-list.txt) with the mapping between the pagelet uri and the file.
	
	* PageletTimingsStatisticsReporter: pagelet-stats.xls

	  Writers various statistical parameters per pagename of the pagelets called.
	  The statistics produced are number of invocations, average, min, max and standard-deviation for the download time.
	
	* PageCriteriaReporter: pagecriteria.txt

	  If this pagelet is configured to be cached, this report will report on arguments used to call the pagelet that are not part of the pagelet cache criteria.
	  
	* PageRenderTimeReporter: pagelet-timings.txt

	  Reports for each pagelet call done  what the pagename, download time, http response code and the uri was.
	
	* RootElementReporter: root-elements.txt

	  Reports the root elements for all the pagelets invoked.
	  
	* NumberOfInnerPageletsReporter: num-inner-pagelets.txt 

	  Writers a list of (direct) inner pagelets per invoked pagelet if the number of inner pagelets is higher then 12.
	                
	* Non200ResponseCodeReporter: non-200-repsonse.txt

	  Which pagelets produce a non 200 HTTP status code response. A non-200 response is considered an error.
	
	* InnerPageletPerOuterReporter: inner-pagelets.txt

	  Reports per page what the innner pagelets are, as well as their nesting.
	
	* InnerLinkReporter: inner-links.txt

	  Reports per pagelet what the links are (from render:getpageurl, render:getbloburl etc) are.
	
	* NestingReporter: nesting.txt

	  Reports per pagelet the number of direct and nested pagelets if this number is higher then 10.
	  
	* PageletOnlyReporter: pagelet-only.txt

	  Reports a list of pagelets that can not be invoked externally. (Have site catalog pageletonly flag set to T).
	
	* NotCachedReporter: not-cached.txt

	  Reports a list of pagelets that are not cached, either by configuration or by runtime.
	
	* DefaultArgumentsAsPageCriteriaReporter: defaultArgumentsAsPageCriteriaReporter.txt

	  Reports if the pagelet default arguments (as per SiteCatalog) are not also cache criteria.

